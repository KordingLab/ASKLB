{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/pyparsing.py:2910: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile( self.reString )\n"
     ]
    }
   ],
   "source": [
    "# this code generate synthetic dataset for machine learning\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import randint\n",
    "import scipy.stats as ss\n",
    "import copy\n",
    "\n",
    "# AutoSKLearn imports\n",
    "import autosklearn.classification\n",
    "from statistics import mean\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "n_feat = 3000\n",
    "n_sample = 300\n",
    "n_query = 20\n",
    "n_substitute = 2\n",
    "t_query = 2; #time budget given to each query (both informative-part-search and actual query)\n",
    "fold = 3\n",
    "\n",
    "threshold1 = 0.1 # For applying thresholdout to panel screening \n",
    "noise1 = 0.03 #scale of laplacian noise used for determining if a part is informative\n",
    "threshold2 = 0.05; #Thresholdout only! used for determining if query result is acc_test or acc_train\n",
    "noise2 = 0.01; #Thresholdout only! scale of laplacian noise used for determining if query result is acc_test or acc_train\n",
    "noise3 = 0.01; #Thresholdout only! scale of laplacian noise applied in reported query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make synthetic data 1: 10000fet~300samples (100train 100test and 100fresh test), 100 infromative features\n",
    "# a = sklearn.datasets.make_classification(n_samples=n_sample, n_features=n_feat, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.25, class_sep=4.0, hypercube=True, shift=None, scale=None, shuffle=True, random_state=None)\n",
    "# X = a[0] # data part of the dataset\n",
    "# X_train =X[0:100] # training set contains 100 samples\n",
    "# X_test =X[100:200] # testing set contains 100 samples\n",
    "# X_fresh =X[200:300] # fresh test set contains 100 samples\n",
    "# y = a[1] # label of all the samples\n",
    "# y_train = np.asarray(y[0:100]) # training set contains 100 samples\n",
    "# y_test = np.asarray(y[100:200]) # testing set contains 100 samples\n",
    "# y_fresh = np.asarray(y[200:300]) # fresh test set contains 100 samples\n",
    "# X_train = pd.DataFrame(X_train) # transfer to pandas dataframe\n",
    "# X_test = pd.DataFrame(X_test)\n",
    "# X_fresh = pd.DataFrame(X_fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make synthetic data 2: 10000fet~300samples(100train 100test and 100fresh test), no infromative features\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "X = []; # data portion of the synthetic dataset\n",
    "for x in range(0, n_feat):\n",
    "    s = np.random.normal(mu, sigma, n_sample)\n",
    "    X.append(s)\n",
    "X = pd.DataFrame(X)\n",
    "X = X.T\n",
    "X_train =X[0:int(n_sample/3)] # training set contains 1/3 samples\n",
    "X_test =X[int(n_sample/3):int(2*n_sample/3)] # testing set contains 1/3 samples\n",
    "X_fresh =X[int(2*n_sample/3):int(n_sample)] # fresh test set contains 1/3 samples\n",
    "y = np.random.normal(mu, sigma, n_sample) # create a evenly balanced label (of 0,1) of length 300\n",
    "# import matplotlib.pyplot as plt \n",
    "# count, bins, ignored = plt.hist(y, 30, normed=True)\n",
    "# plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *np.exp( - (bins - mu)**2 / (2 * sigma**2) ),linewidth=2, color='r')\n",
    "# plt.show()\n",
    "y[y>=0] = 1 #50% with label 1\n",
    "y[y<0] = 0 #50% with label 0\n",
    "y_new = [int(i) for i in y] # convert label to integer\n",
    "y_train = np.asarray(y_new[0:int(n_sample/3)]) # training set contains 1/3 samples\n",
    "y_test = np.asarray(y_new[int(n_sample/3):int(2*n_sample/3)]) # testing set contains 1/3 samples\n",
    "y_fresh = np.asarray(y_new[int(2*n_sample/3):int(n_sample)]) # fresh test set contains 1/3 samples\n",
    "\n",
    "# y_train\n",
    "# X_fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [11, 12],\n",
       " [13, 14],\n",
       " [15, 16],\n",
       " [17, 18],\n",
       " [19, 20],\n",
       " [21, 22],\n",
       " [23, 24],\n",
       " [25, 26],\n",
       " [27, 28],\n",
       " [29, 30],\n",
       " [31, 32],\n",
       " [33, 34],\n",
       " [35, 36],\n",
       " [37, 38],\n",
       " [39, 40],\n",
       " [41, 42],\n",
       " [43, 44],\n",
       " [45, 46],\n",
       " [47, 48],\n",
       " [49, 50]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define panel indices\n",
    "ind_select = [] # index of randomly chosen features for each run\n",
    "ind_select.append([*range(10)]) # starting with the first 10 features\n",
    "for i in range(1,n_query+1): #gice Thonak\n",
    "    inds = [*range(i*n_substitute+9, (i+1)*n_substitute+9)] # try switch 5 each time\n",
    "    ind_select.append(inds)\n",
    "\n",
    "ind_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2019-10-13 20:28:45,130:EnsembleBuilder(1):7fdbf792db4e3d81927ea15ad697c0ec] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-10-13 20:28:45,146:EnsembleBuilder(1):7fdbf792db4e3d81927ea15ad697c0ec] No models better than random - using Dummy Score!\n",
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (119.518821)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "Process pynisher function call:\n",
      "Process pynisher function call:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
      "    return_value = ((func(*args, **kwargs), 0))\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/ensemble_builder.py\", line 234, in main\n",
      "    time.sleep(self.sleep_duration)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
      "    return_value = ((func(*args, **kwargs), 0))\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/__init__.py\", line 30, in fit_predict_try_except_decorator\n",
      "    return ta(queue=queue, **kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py\", line 648, in eval_holdout\n",
      "    evaluator.fit_predict_and_loss(iterative=iterative)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py\", line 160, in fit_predict_and_loss\n",
      "    i, train_indices=train_split, test_indices=test_split\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py\", line 306, in _partial_fit_and_predict\n",
      "    model = self._get_model()\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/abstract_evaluator.py\", line 196, in _get_model\n",
      "    init_params=self._init_params)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/classification.py\", line 83, in __init__\n",
      "    random_state, init_params)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/base.py\", line 36, in __init__\n",
      "    self.config_space = self.get_hyperparameter_search_space()\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/base.py\", line 216, in get_hyperparameter_search_space\n",
      "    dataset_properties=self.dataset_properties_)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/classification.py\", line 181, in _get_hyperparameter_search_space\n",
      "    exclude=exclude, include=include, pipeline=self.steps)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/base.py\", line 325, in _get_base_search_space\n",
      "    dataset_properties, include=choices_list)\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/pipeline/components/data_preprocessing/rescaling/__init__.py\", line 63, in get_hyperparameter_search_space\n",
      "    parent_hyperparameter=parent_hyperparameter)\n",
      "  File \"ConfigSpace/configuration_space.py\", line 519, in ConfigSpace.configuration_space.ConfigurationSpace.add_configuration_space\n",
      "  File \"ConfigSpace/configuration_space.py\", line 240, in ConfigSpace.configuration_space.ConfigurationSpace.add_conditions\n",
      "  File \"ConfigSpace/configuration_space.py\", line 326, in ConfigSpace.configuration_space.ConfigurationSpace._check_edges\n",
      "  File \"ConfigSpace/configuration_space.py\", line 441, in ConfigSpace.configuration_space.ConfigurationSpace._create_tmp_dag\n",
      "  File \"/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/ConfigSpace/nx/classes/digraph.py\", line 171, in __init__\n",
      "    def __init__(self, data=None, **attr):\n",
      "KeyboardInterrupt\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2019-10-13 20:29:02,463:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2019-10-13 20:29:02,463:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[20:29:02] /workspace/src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
      "/home/tliu/.conda/envs/shap/lib/python3.7/site-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
      "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# for each query, substitute the 5 features with the lowest cost function (delta Acc) with the next 5 unselected featurea\n",
    "inds = copy.deepcopy(ind_select[0]); # starting with the first 10 features\n",
    "acc_trains = [] # list of test set accuracy for all subsets, standard resuse\n",
    "acc_tests = [] # list of test set accuracy for all subsets, standard resuse\n",
    "acc_freshs = [] # list of test set accuracy for all subsets, Thresholdout\n",
    "for i in range(n_query):\n",
    "    part_train = X_train.iloc[:, inds]\n",
    "    part_test = X_test.iloc[:, inds]\n",
    "    part_fresh = X_fresh.iloc[:, inds]\n",
    "    sum_acc_train = 0\n",
    "    sum_acc_test1 = 0\n",
    "    sum_acc_test = 0\n",
    "    sum_acc_fresh = 0\n",
    "    \n",
    "    # 3-fold prediction on the panel\n",
    "    for folds in range(fold): \n",
    "        cls1 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls1 = LinearSVC()\n",
    "        cls1.fit(part_train, y_train)\n",
    "        y_test_predict = cls1.predict(part_test); # predicted label of test set\n",
    "        sum_acc_test1 += sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "    acc_test1 = sum_acc_test1/fold\n",
    "    \n",
    "    # leave One Out calculation of loss function\n",
    "    acc_test_loss = [] # accuracy of 9 feature subpanels to calculate\n",
    "    for j in range(10): # each time take out one feature and calculate the new accracies and corresponding loss function\n",
    "        inds_loss = copy.deepcopy(inds)\n",
    "        inds_loss.remove(inds[j])\n",
    "        part_train_loss = X_train.iloc[:, inds_loss]\n",
    "        part_test_loss = X_test.iloc[:, inds_loss]\n",
    "        cls2 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls2 = LinearSVC()\n",
    "        cls2.fit(part_train_loss, y_train)\n",
    "        y_test_predict = cls2.predict(part_test_loss); # predicted label of test set\n",
    "        acc_test2 = sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "        acc_test_loss.append(acc_test1 - acc_test2)\n",
    "        \n",
    "    # find the lowest n loss functions and substitute with new features\n",
    "    ranking =  sorted(range(len(acc_test_loss)), key=lambda k: acc_test_loss[k]) # index rankning (low to high of the loss functions)\n",
    "    toRemove = [] # n indices to remove from the current panel\n",
    "    for k in range(n_substitute):\n",
    "        toRemove.append(inds[ranking[k]])\n",
    "    inds = list(set(inds).difference(set(toRemove)))\n",
    "    for folds in range(fold): \n",
    "        cls3 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls3 = LinearSVC()\n",
    "        cls3.fit(part_train, y_train)\n",
    "        y_train_predict = cls3.predict(part_train); # predicted label of training set\n",
    "        sum_acc_train += sklearn.metrics.accuracy_score(y_train_predict, y_train)\n",
    "        y_test_predict = cls3.predict(part_test); # predicted label of test set\n",
    "        sum_acc_test += sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "        y_fresh_predict = cls3.predict(part_fresh); # predicted label of fresh set\n",
    "        sum_acc_fresh += sklearn.metrics.accuracy_score(y_fresh_predict, y_fresh)\n",
    "    acc_train = sum_acc_train/fold\n",
    "    acc_test = sum_acc_test/fold\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n",
    "    acc_freshs.append(sum_acc_fresh/fold)    \n",
    "    \n",
    "    with open('ThresholdOutReport_substitution_2', \"a+\") as myfile:\n",
    "        myfile.write(\"\\n query number: \"+str(i))\n",
    "        myfile.write(\"\\n acc_trains = : \"+str(acc_trains))\n",
    "        myfile.write(\"\\n acc_tests = : \"+str(acc_tests))\n",
    "        myfile.write(\"\\n acc_freshs = : \"+str(acc_freshs))\n",
    "        myfile.write(\"\\n acc_test_loss = : \"+str(acc_test_loss))\n",
    "        myfile.write(\"\\n indices of features in panel = : \"+str(inds))\n",
    "        \n",
    "\n",
    "    #Now insert the next three features\n",
    "    inds.extend(ind_select[i+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ThresholdOut\n",
    "# for each query, substitute the 3 features with the lowest cost function (delta Acc) with the next 5 unselected featurea\n",
    "inds = copy.deepcopy(ind_select[0]); # starting with the first 10 features\n",
    "acc_trains = [] # list of test set accuracy for all subsets, standard resuse\n",
    "acc_tests = [] # list of test set accuracy for all subsets, standard resuse\n",
    "acc_freshs = [] # list of test set accuracy for all subsets, Thresholdout\n",
    "for i in range(n_query):\n",
    "    part_train = X_train.iloc[:, inds]\n",
    "    part_test = X_test.iloc[:, inds]\n",
    "    part_fresh = X_fresh.iloc[:, inds]\n",
    "    sum_acc_train = 0\n",
    "    sum_acc_test1 = 0\n",
    "    sum_acc_test = 0\n",
    "    sum_acc_fresh = 0\n",
    "    \n",
    "    # 3-fold prediction on the panel\n",
    "    for folds in range(fold): \n",
    "        cls1 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls1 = LinearSVC()\n",
    "        cls1.fit(part_train, y_train)\n",
    "        y_test_predict = cls1.predict(part_test); # predicted label of test set\n",
    "        sum_acc_test1 += sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "    acc_test1 = sum_acc_test1/fold\n",
    "    \n",
    "    # leave One Out calculation of loss function\n",
    "    acc_test_loss = [] # accuracy of 9 feature subpanels to calculate\n",
    "    for j in range(10): # each time take out one feature and calculate the new accracies and corresponding loss function\n",
    "        inds_loss = copy.deepcopy(inds)\n",
    "        inds_loss.remove(inds[j])\n",
    "        part_train_loss = X_train.iloc[:, inds_loss]\n",
    "        part_test_loss = X_test.iloc[:, inds_loss]\n",
    "        cls2 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls2 = LinearSVC()\n",
    "        cls2.fit(part_train_loss, y_train)\n",
    "        y_train_predict = cls2.predict(part_train_loss); # predicted label of test set\n",
    "        acc_train2 = sklearn.metrics.accuracy_score(y_train_predict, y_train)\n",
    "        y_test_predict = cls2.predict(part_test_loss); # predicted label of test set\n",
    "        acc_test2 = sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "        if acc_train2 - acc_test2 < threshold1 + np.random.laplace(0, noise1): #apply thresholdout to query output\n",
    "            acc_test2 = acc_train2\n",
    "        else:\n",
    "            acc_test2 = acc_test2 + np.random.laplace(0, noise1)\n",
    "        acc_test_loss.append(acc_test1 - acc_test2)\n",
    "        \n",
    "    # find the lowest n loss functions and substitute with new features\n",
    "    ranking =  sorted(range(len(acc_test_loss)), key=lambda k: acc_test_loss[k]) # index rankning (low to high of the loss functions)\n",
    "    toRemove = [] # n indices to remove from the current panel\n",
    "    for k in range(n_substitute):\n",
    "        toRemove.append(inds[ranking[k]])\n",
    "    inds = list(set(inds).difference(set(toRemove)))\n",
    "    \n",
    "    for folds in range(fold): \n",
    "        cls3 = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = t_query*60)\n",
    "#         cls3 = LinearSVC()\n",
    "        cls3.fit(part_train, y_train)\n",
    "        y_train_predict = cls3.predict(part_train); # predicted label of training set\n",
    "        sum_acc_train += sklearn.metrics.accuracy_score(y_train_predict, y_train)\n",
    "        y_test_predict = cls3.predict(part_test); # predicted label of test set\n",
    "        sum_acc_test += sklearn.metrics.accuracy_score(y_test_predict, y_test)\n",
    "        y_fresh_predict = cls3.predict(part_fresh); # predicted label of fresh set\n",
    "        sum_acc_fresh += sklearn.metrics.accuracy_score(y_fresh_predict, y_fresh)\n",
    "    acc_train = sum_acc_train/fold\n",
    "    acc_test = sum_acc_test/fold\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n",
    "    acc_freshs.append(sum_acc_fresh/fold)    \n",
    "    \n",
    "    with open('ThresholdOutReport_substitution_2_TO', \"a+\") as myfile:\n",
    "        myfile.write(\"\\n query number: \"+str(i))\n",
    "        myfile.write(\"\\n acc_trains = : \"+str(acc_trains))\n",
    "        myfile.write(\"\\n acc_tests = : \"+str(acc_tests))\n",
    "        myfile.write(\"\\n acc_freshs = : \"+str(acc_freshs))\n",
    "        myfile.write(\"\\n acc_test_loss = : \"+str(acc_test_loss))\n",
    "        myfile.write(\"\\n indices of features in panel = : \"+str(inds))\n",
    "        \n",
    "\n",
    "    #Now insert the next three features\n",
    "    inds.extend(ind_select[i+1])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save the synthetic data \n",
    "# dump_file = 'ThresholdoutData_X.dump.pkl'\n",
    "# with open(dump_file, 'wb') as f:\n",
    "#     pickle.dump(X, f)\n",
    "#     f.close()\n",
    "# dump_file = 'ThresholdoutData_y.dump.pkl'\n",
    "# with open(dump_file, 'wb') as f:\n",
    "#     pickle.dump(y, f)\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load file\n",
    "import pickle\n",
    "# f = open('ThresholdoutData_1.dump.pkl', \"rb\")\n",
    "# a = pickle.load(f)\n",
    "# f.close()\n",
    "# X = a[0] # data part of the dataset\n",
    "\n",
    "f = open('ThresholdoutData_X.dump.pkl', \"rb\")\n",
    "X = pickle.load(f)\n",
    "f.close()\n",
    "f = open('ThresholdoutData_y.dump.pkl', \"rb\")\n",
    "y = pickle.load(f)\n",
    "f.close()\n",
    "X_train =X[0:100] # training set contains 100 samples\n",
    "X_test =X[100:200] # testing set contains 100 samples\n",
    "X_fresh =X[200:300] # fresh test set contains 100 samples\n",
    "y_train = np.asarray(y[0:100]) # training set contains 100 samples\n",
    "y_test = np.asarray(y[100:200]) # testing set contains 100 samples\n",
    "y_fresh = np.asarray(y[200:300]) # fresh test set contains 100 samples\n",
    "X_train = pd.DataFrame(X_train) # transfer to pandas dataframe\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_fresh = pd.DataFrame(X_fresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
